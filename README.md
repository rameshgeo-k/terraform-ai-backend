# Terraform AI Assistant API

Production-ready FastAPI application with CodeLlama-7b and RAG capabilities.

## ğŸ—ï¸ Project Structure

```
server/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py           # Application package
â”‚   â”œâ”€â”€ main.py               # FastAPI application entry point
â”‚   â”œâ”€â”€ api/                  # API layer
â”‚   â”‚   â””â”€â”€ v1/               # API version 1
â”‚   â”‚       â”œâ”€â”€ router.py     # Main API router
â”‚   â”‚       â””â”€â”€ endpoints/    # Endpoint modules
â”‚   â”‚           â”œâ”€â”€ health.py # Health check
â”‚   â”‚           â”œâ”€â”€ chat.py   # Chat completion
â”‚   â”‚           â””â”€â”€ rag.py    # RAG operations
â”‚   â”œâ”€â”€ core/                 # Core functionality
â”‚   â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”‚   â””â”€â”€ logging.py        # Logging setup
â”‚   â”œâ”€â”€ schemas/              # Pydantic schemas
â”‚   â”‚   â””â”€â”€ requests.py       # Request/Response models
â”‚   â”œâ”€â”€ services/             # Business logic
â”‚   â”‚   â”œâ”€â”€ ollama.py         # Ollama service
â”‚   â”‚   â””â”€â”€ rag.py            # RAG service
â”‚   â””â”€â”€ utils/                # Utility functions
â”‚       â””â”€â”€ helpers.py        # Helper functions
â”œâ”€â”€ config/                   # Configuration files
â”‚   â””â”€â”€ server_config.yaml    # Server settings
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ logs/                     # Application logs
â”œâ”€â”€ chroma/                   # Vector database
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ .gitignore                # Git ignore rules
â””â”€â”€ README.md                 # This file
```

## âœ¨ Features

- **OpenAI-Compatible API**: Drop-in replacement for OpenAI's chat API
- **Streaming Support**: Real-time token streaming for chat responses
- **RAG Integration**: Semantic search with ChromaDB
- **Production-Ready**: Structured logging, error handling, validation
- **Type Safety**: Full Pydantic validation
- **API Versioning**: Clean v1 API structure
- **Dependency Injection**: FastAPI dependencies for services
- **Configuration Management**: YAML + environment variables

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Ollama (will be installed by setup script if needed)
- CodeLlama-7b-Terraform-Merged-f16.gguf model file

### Automated Setup (Recommended)

```bash
# 1. Ensure GGUF model is in parent directory
# /home/nova/AI/CodeLlama-7b-Terraform-Merged-f16.gguf

# 2. Run setup script
cd /home/nova/AI/server
./setup.sh
```

The setup script will:
- Install Ollama (if not already installed)
- Create Python virtual environment
- Install dependencies
- Import model to Ollama
- Verify everything is working

### Manual Installation

1. **Clone and navigate**:
   ```bash
   cd /home/nova/AI/server
   ```

2. **Download model** (see `MODEL_SETUP.md`):
   ```bash
   # Place GGUF file in parent directory
   # /home/nova/AI/CodeLlama-7b-Terraform-Merged-f16.gguf
   ```

3. **Create virtual environment**:
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

4. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

5. **Import model to Ollama**:
   ```bash
   cd ..
   ollama create terraform-codellama -f server/Modelfile
   cd server
   ```

6. **Configure environment** (optional):
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

7. **Run the server**:
   ```bash
   python -m app.main
   ```

   Or with uvicorn directly:
   ```bash
   uvicorn app.main:app --host 0.0.0.0 --port 8000
   ```

## ğŸ“‹ API Endpoints

### Health Check

```bash
GET /health
```

Returns service status:
```json
{
  "status": "healthy",
  "ollama_connected": true,
  "rag_initialized": true
}
```

### Chat Completion

```bash
POST /v1/chat/completions
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "How do I create a VPC in Terraform?"}
  ],
  "temperature": 0.7,
  "max_tokens": 512,
  "stream": false
}
```

### RAG Operations

**Add Document**:
```bash
POST /v1/rag/documents
{
  "text": "Terraform best practices...",
  "metadata": {"category": "terraform"}
}
```

**Query Documents**:
```bash
POST /v1/rag/query
{
  "query": "VPC configuration",
  "top_k": 3
}
```

**RAG-Enhanced Chat**:
```bash
POST /v1/rag/chat
{
  "query": "How do I configure a VPC?",
  "top_k": 3,
  "temperature": 0.7
}
```

## âš™ï¸ Configuration

### YAML Configuration (`config/server_config.yaml`)

```yaml
server:
  host: "0.0.0.0"
  port: 8000
  
model:
  ollama_base_url: "http://localhost:11434"
  model_name: "terraform-codellama"
  
inference:
  default_max_tokens: 512
  default_temperature: 0.7
  
rag:
  chroma_persist_directory: "./chroma"
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
```

### Environment Variables

All settings can be overridden with environment variables. See `.env.example`.

## ğŸ§ª Testing

Run tests with pytest:

```bash
pytest tests/ -v
```

## ğŸ“Š Logging

Logs are stored in `logs/`:
- `requests.log` - All API requests
- `errors.log` - Error-level logs

JSON structured logging enabled by default.

## ğŸ”§ Development

### Install development dependencies:

```bash
pip install -r requirements.txt
```

### Run with auto-reload:

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Code Organization Best Practices

- **Schemas** (`app/schemas/`): Pydantic models for validation
- **Services** (`app/services/`): Business logic (Ollama, RAG)
- **Endpoints** (`app/api/v1/endpoints/`): Route handlers
- **Core** (`app/core/`): Configuration, logging, utilities
- **Utils** (`app/utils/`): Helper functions

## ğŸ³ Docker (Coming Soon)

```bash
docker build -t terraform-ai-assistant .
docker run -p 8000:8000 terraform-ai-assistant
```

## ğŸ“ License

MIT

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“š Documentation

- FastAPI docs: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc
